{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/czimbermark/Documents/Egyetem/Adatelemzes/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/czimbermark/Documents/Egyetem/Adatelemzes/Nagyhazi/FungiCLEF2024_ADC\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import timm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "os.chdir(\"/Users/czimbermark/Documents/Egyetem/Adatelemzes/Nagyhazi/FungiCLEF2024_ADC/\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset2 import FungiDataset\n",
    "from src.dataset2 import fungi_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_loader: 644 and val_loader: 161\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    \"image_dir\": \"/Users/czimbermark/Documents/Egyetem/Adatelemzes/Nagyhazi/FungiCLEF2024_ADC/data/x_train\",\n",
    "    \"labels_path\": \"/Users/czimbermark/Documents/Egyetem/Adatelemzes/Nagyhazi/FungiCLEF2024_ADC/data/train_metadata_height.csv\",\n",
    "    \"pre_load\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"crop_height\": 16,\n",
    "    \"interpolate\": \"bilinear\",\n",
    "    \"out_size\": (224, 224)\n",
    "}\n",
    "\n",
    "# Define the classes to include (your list of unique class IDs)\n",
    "class_ids_to_include = [4, 11, 16, 25, 30, 32, 37, 39, 43, 63, 100, 103, 128, 129, 131, 136, 142, 168, 180, 213, 214, 223, 252, 266, 309, 366, 389, 413, 473, 478, 487, 522, 555, 559, 591, 633, 637, 657, 671, 673, 689, 694, 724, 728, 738, 748, 764, 787, 812, 814, 830, 837, 845, 856, 884, 908, 909, 912, 967, 975, 989, 992, 1000, 1005, 1014, 1020, 1052, 1054, 1088, 1093, 1115, 1121, 1135, 1136, 1141, 1160, 1183, 1207, 1214, 1220, 1221, 1232, 1239, 1242, 1290, 1302, 1355, 1381, 1395, 1420, 1438, 1440, 1481, 1484, 1493, 1533, 1537, 1546, 1573, 1603]\n",
    "\n",
    "# Initialize the full dataset\n",
    "full_dataset = FungiDataset(\n",
    "    image_dir=config[\"image_dir\"],\n",
    "    labels_path=config[\"labels_path\"],\n",
    "    pre_load=config[\"pre_load\"],\n",
    "    crop_height=config[\"crop_height\"],\n",
    "    interpolate=config[\"interpolate\"],\n",
    "    out_size=config[\"out_size\"],\n",
    "    transform=None,\n",
    "    class_ids_to_include=class_ids_to_include  # Pass the list of class IDs\n",
    ")\n",
    "\n",
    "print(f\"Number of samples in dataset: {len(full_dataset)}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(full_dataset))), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    prefetch_factor=0,  # Prefetch batches to improve performance\n",
    "    pin_memory=True,  # Optimize for GPU\n",
    "    collate_fn=fungi_collate_fn, \n",
    "    drop_last=True  # Drops the last incomplete batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    prefetch_factor=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=fungi_collate_fn,\n",
    "    drop_last=True # also\n",
    ")\n",
    "\n",
    "print(f\"Length of train_loader: {len(train_loader)} and val_loader: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of species classes: 100\n",
      "MultiTaskModel(\n",
      "  (base_model): MetaFormer(\n",
      "    (stem): Stem(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
      "      (norm): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): MetaFormerStage(\n",
      "        (downsample): Identity()\n",
      "        (blocks): Sequential(\n",
      "          (0): MetaFormerBlock(\n",
      "            (norm1): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): SepConv(\n",
      "              (pwconv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act1): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
      "              (act2): Identity()\n",
      "              (pwconv2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Identity()\n",
      "            (norm2): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Identity()\n",
      "          )\n",
      "          (1): MetaFormerBlock(\n",
      "            (norm1): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): SepConv(\n",
      "              (pwconv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act1): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
      "              (act2): Identity()\n",
      "              (pwconv2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Identity()\n",
      "            (norm2): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Identity()\n",
      "          )\n",
      "          (2): MetaFormerBlock(\n",
      "            (norm1): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): SepConv(\n",
      "              (pwconv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act1): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
      "              (act2): Identity()\n",
      "              (pwconv2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Identity()\n",
      "            (norm2): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): MetaFormerStage(\n",
      "        (downsample): Downsampling(\n",
      "          (norm): LayerNorm2dNoBias((64,), eps=1e-06, elementwise_affine=True)\n",
      "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): MetaFormerBlock(\n",
      "            (norm1): LayerNorm2dNoBias((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): SepConv(\n",
      "              (pwconv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act1): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
      "              (act2): Identity()\n",
      "              (pwconv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Identity()\n",
      "            (norm2): LayerNorm2dNoBias((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Identity()\n",
      "          )\n",
      "          (1): MetaFormerBlock(\n",
      "            (norm1): LayerNorm2dNoBias((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): SepConv(\n",
      "              (pwconv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act1): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
      "              (act2): Identity()\n",
      "              (pwconv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Identity()\n",
      "            (norm2): LayerNorm2dNoBias((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Identity()\n",
      "          )\n",
      "          (2): MetaFormerBlock(\n",
      "            (norm1): LayerNorm2dNoBias((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): SepConv(\n",
      "              (pwconv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act1): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256, bias=False)\n",
      "              (act2): Identity()\n",
      "              (pwconv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Identity()\n",
      "            (norm2): LayerNorm2dNoBias((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): MetaFormerStage(\n",
      "        (downsample): Downsampling(\n",
      "          (norm): LayerNorm2dNoBias((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (conv): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (1): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (2): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (3): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (4): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (5): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (6): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (7): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (8): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=320, out_features=960, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=320, out_features=320, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=320, out_features=1280, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=1280, out_features=320, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): MetaFormerStage(\n",
      "        (downsample): Downsampling(\n",
      "          (norm): LayerNorm2dNoBias((320,), eps=1e-06, elementwise_affine=True)\n",
      "          (conv): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "        (blocks): Sequential(\n",
      "          (0): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (1): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "          (2): MetaFormerBlock(\n",
      "            (norm1): LayerNormNoBias((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (token_mixer): Attention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path1): Identity()\n",
      "            (layer_scale1): Identity()\n",
      "            (res_scale1): Scale()\n",
      "            (norm2): LayerNormNoBias((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (act): StarReLU(\n",
      "                (relu): ReLU()\n",
      "              )\n",
      "              (drop1): Dropout(p=0.0, inplace=False)\n",
      "              (norm): Identity()\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (drop2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (drop_path2): Identity()\n",
      "            (layer_scale2): Identity()\n",
      "            (res_scale2): Scale()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (head): Sequential(\n",
      "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "      (norm): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (fc): MlpHead(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): SquaredReLU(\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "        (head_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_toxicity): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set num_species_classes based on the dataset\n",
    "num_species_classes = full_dataset.num_species_classes\n",
    "print(f\"Number of species classes: {num_species_classes}\")  # Should print 100\n",
    "num_toxicity_classes = 2   # Assuming binary classification (edible or poisonous)\n",
    "\n",
    "# Create the CAFormer model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model = timm.create_model(\n",
    "    \"caformer_s18.sail_in22k\",\n",
    "    pretrained=True,\n",
    "    num_classes=num_species_classes  # For species classification\n",
    ")\n",
    "base_model.to(device)\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model, num_species_classes, num_toxicity_classes):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        # Replace the classifier for species classification\n",
    "        self.base_model.reset_classifier(num_species_classes)\n",
    "\n",
    "        num_features = base_model.num_features  # Number of features after pooling\n",
    "\n",
    "        # Create a new classifier for toxicity\n",
    "        self.fc_toxicity = nn.Linear(num_features, num_toxicity_classes)\n",
    "\n",
    "        # Define a pooling layer if not present\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.base_model.forward_features(x)  # Shape: [batch_size, channels, height, width]\n",
    "        #Â print(f\"Features shape: {features.shape}\")\n",
    "\n",
    "        # Species classification (using base model's head)\n",
    "        species_logits = self.base_model.forward_head(features)\n",
    "        # print(f\"Species logits shape: {species_logits.shape}\")\n",
    "\n",
    "        # Apply global average pooling for toxicity classification\n",
    "        pooled_features = self.global_pool(features).flatten(1)  # Shape: [batch_size, channels]\n",
    "        # print(f\"Pooled features shape: {pooled_features.shape}\")\n",
    "\n",
    "        # Toxicity classification\n",
    "        toxicity_logits = self.fc_toxicity(pooled_features)\n",
    "        # print(f\"Toxicity logits shape: {toxicity_logits.shape}\")\n",
    "\n",
    "        return {'species': species_logits, 'toxicity': toxicity_logits}\n",
    "\n",
    "# Initialize the multi-task model\n",
    "model = MultiTaskModel(base_model, num_species_classes, num_toxicity_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adjust learning rate as needed\n",
    "\n",
    "# Define loss functions\n",
    "criterion_species = nn.CrossEntropyLoss()\n",
    "criterion_toxicity = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class IDs in batch: tensor([17, 47, 69,  6, 81, 23, 98, 91, 93, 50, 89, 85, 56, 91, 34, 78, 12,  5,\n",
      "        41, 88, 80, 47, 12, 33, 41, 44, 53, 86, 63, 71, 39, 40])\n",
      "Min class ID: 5, Max class ID: 98\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of data\n",
    "images, (class_ids, toxicities), _ = next(iter(train_loader))\n",
    "\n",
    "# Check the range of class_ids\n",
    "print(f\"Class IDs in batch: {class_ids}\")\n",
    "print(f\"Min class ID: {class_ids.min()}, Max class ID: {class_ids.max()}\")\n",
    "\n",
    "# Ensure class IDs are in the range [0, num_species_classes - 1]\n",
    "assert class_ids.min() >= 0 and class_ids.max() < num_species_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(targets, predictions, task_name, matrix_type='species', colors=None):\n",
    "    \"\"\"\n",
    "    Compute and plot a confusion matrix with customizable display and colors.\n",
    "\n",
    "    Args:\n",
    "        targets: True labels.\n",
    "        predictions: Predicted labels.\n",
    "        task_name: Name of the task (e.g., 'Species', 'Toxicity').\n",
    "        matrix_type: Type of matrix ('species' or 'toxicity') for specific display logic.\n",
    "        colors: Custom color palette.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import numpy as np\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(targets, predictions)\n",
    "\n",
    "    if matrix_type == 'species':\n",
    "        # For Species Misclassification Matrix\n",
    "        plt.figure(figsize=(6, 1))  # Wide and short, single row\n",
    "        sns.set(font_scale=1.4)\n",
    "\n",
    "        # Define colors: Red for misclassified, Green for recognized\n",
    "        cmap = sns.color_palette(['green', 'red']) if not colors else sns.color_palette(colors)\n",
    "\n",
    "        sns.heatmap(\n",
    "            cm[:1],  # Only show the first row\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap=cmap,\n",
    "            cbar=False,\n",
    "            xticklabels=['Missclassified', 'Recognized'],  # Bottom labels\n",
    "            yticklabels=[]  # Remove true labels from the side\n",
    "        )\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.title(f'{task_name} Misclassification Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    if matrix_type == 'toxicity':\n",
    "        # For Toxicity Matrix\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.set(font_scale=1.4)\n",
    "\n",
    "        # Assign unique indices to each cell type\n",
    "        cell_type_indices = np.array([[0, 1], [2, 3]])  # TN, FP, FN, TP\n",
    "\n",
    "        # Define custom colors for each cell type\n",
    "        if colors is None:\n",
    "            colors = ['black', 'yellow', 'purple', 'black']  # TN, FP, FN, TP\n",
    "\n",
    "        # Create a custom colormap from the specified colors\n",
    "        custom_cmap = ListedColormap(colors)\n",
    "\n",
    "        # Plot the heatmap using the cell_type_indices to map colors\n",
    "        sns.heatmap(\n",
    "            cell_type_indices,\n",
    "            annot=cm,\n",
    "            fmt='d',\n",
    "            cmap=custom_cmap,\n",
    "            cbar=False,\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive']\n",
    "        )\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'{task_name} Classification Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=10, alpha = 1.0, beta = 0.6):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies_species = []\n",
    "    train_accuracies_toxicity = []\n",
    "    val_accuracies_species = []\n",
    "    val_accuracies_toxicity = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_species = 0\n",
    "        correct_toxicity = 0\n",
    "        total_samples = 0\n",
    "    \n",
    "        # Iterate over batches\n",
    "        for batch_idx, (images, (class_ids, toxicities), _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            class_ids = class_ids.to(device, dtype=torch.long)\n",
    "            toxicities = toxicities.to(device, dtype=torch.long)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "    \n",
    "            loss_species = criterion_species(outputs['species'], class_ids)\n",
    "            loss_toxicity = criterion_toxicity(outputs['toxicity'], toxicities)\n",
    "            loss = alpha * loss_species + beta * loss_toxicity  # weighting\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Update running loss\n",
    "            batch_loss = loss.item() * images.size(0)\n",
    "            running_loss += batch_loss\n",
    "    \n",
    "            # Calculate accuracies\n",
    "            _, preds_species = torch.max(outputs['species'], 1)\n",
    "            _, preds_toxicity = torch.max(outputs['toxicity'], 1)\n",
    "            batch_correct_species = (preds_species == class_ids).sum().item()\n",
    "            batch_correct_toxicity = (preds_toxicity == toxicities).sum().item()\n",
    "            batch_samples = images.size(0)\n",
    "            correct_species += batch_correct_species\n",
    "            correct_toxicity += batch_correct_toxicity\n",
    "            total_samples += batch_samples\n",
    "    \n",
    "            # Calculate batch accuracies\n",
    "            batch_accuracy_species = batch_correct_species / batch_samples * 100\n",
    "            batch_accuracy_toxicity = batch_correct_toxicity / batch_samples * 100\n",
    "    \n",
    "            # Print per-batch metrics\n",
    "            print(f\"Train Batch {batch_idx+1}/{len(train_loader)}: Loss = {loss.item():.4f}, Species Acc = {batch_accuracy_species:.2f}%, Toxicity Acc = {batch_accuracy_toxicity:.2f}%\")\n",
    "    \n",
    "        avg_train_loss = running_loss / total_samples\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracy_species = correct_species / total_samples * 100\n",
    "        train_accuracy_toxicity = correct_toxicity / total_samples * 100\n",
    "        train_accuracies_species.append(train_accuracy_species)\n",
    "        train_accuracies_toxicity.append(train_accuracy_toxicity)\n",
    "    \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct_species = 0\n",
    "        val_correct_toxicity = 0\n",
    "        val_total_samples = 0\n",
    "        val_predictions_species = []\n",
    "        val_targets_species = []\n",
    "        val_predictions_toxicity = []\n",
    "        val_targets_toxicity = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, (class_ids, toxicities), _) in enumerate(val_loader):\n",
    "                images = images.to(device)\n",
    "                class_ids = class_ids.to(device, dtype=torch.long)\n",
    "                toxicities = toxicities.to(device, dtype=torch.long)\n",
    "    \n",
    "                outputs = model(images)\n",
    "    \n",
    "                loss_species = criterion_species(outputs['species'], class_ids)\n",
    "                loss_toxicity = criterion_toxicity(outputs['toxicity'], toxicities)\n",
    "                loss = alpha * loss_species + beta * loss_toxicity  # Adjust weighting\n",
    "    \n",
    "                # Update running loss\n",
    "                batch_loss = loss.item() * images.size(0)\n",
    "                val_running_loss += batch_loss\n",
    "    \n",
    "                # Calculate accuracies\n",
    "                _, preds_species = torch.max(outputs['species'], 1)\n",
    "                _, preds_toxicity = torch.max(outputs['toxicity'], 1)\n",
    "                batch_correct_species = (preds_species == class_ids).sum().item()\n",
    "                batch_correct_toxicity = (preds_toxicity == toxicities).sum().item()\n",
    "                batch_samples = images.size(0)\n",
    "                val_correct_species += batch_correct_species\n",
    "                val_correct_toxicity += batch_correct_toxicity\n",
    "                val_total_samples += batch_samples\n",
    "    \n",
    "                # Store predictions and targets for confusion matrices\n",
    "                val_predictions_species.extend(preds_species.cpu().numpy())\n",
    "                val_targets_species.extend(class_ids.cpu().numpy())\n",
    "                val_predictions_toxicity.extend(preds_toxicity.cpu().numpy())\n",
    "                val_targets_toxicity.extend(toxicities.cpu().numpy())\n",
    "    \n",
    "                # Calculate batch accuracies\n",
    "                batch_accuracy_species = batch_correct_species / batch_samples * 100\n",
    "                batch_accuracy_toxicity = batch_correct_toxicity / batch_samples * 100\n",
    "    \n",
    "                # Print per-batch metrics\n",
    "                print(f\"Val Batch {batch_idx+1}/{len(val_loader)}: Loss = {loss.item():.4f}, Species Acc = {batch_accuracy_species:.2f}%, Toxicity Acc = {batch_accuracy_toxicity:.2f}%\")\n",
    "    \n",
    "        avg_val_loss = val_running_loss / val_total_samples\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracy_species = val_correct_species / val_total_samples * 100\n",
    "        val_accuracy_toxicity = val_correct_toxicity / val_total_samples * 100\n",
    "        val_accuracies_species.append(val_accuracy_species)\n",
    "        val_accuracies_toxicity.append(val_accuracy_toxicity)\n",
    "    \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Species Acc: {train_accuracy_species:.2f}%, Toxicity Acc: {train_accuracy_toxicity:.2f}%\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f}, Species Acc: {val_accuracy_species:.2f}%, Toxicity Acc: {val_accuracy_toxicity:.2f}%\")\n",
    "    \n",
    "        # Confusion Matrices\n",
    "        # For Species Classification\n",
    "        species_correct = np.array(val_predictions_species) == np.array(val_targets_species)\n",
    "        species_binary_predictions = species_correct.astype(int)  # 1 if correct, 0 if incorrect\n",
    "        species_binary_targets = np.zeros_like(species_binary_predictions)  # All zeros (1 row for misclassification matrix)\n",
    "\n",
    "        # Plot Confusion Matrix for Species Classification\n",
    "        conf_matrix(\n",
    "            species_binary_targets, \n",
    "            species_binary_predictions, \n",
    "            task_name='Species', \n",
    "            matrix_type='species', \n",
    "            colors=['red', 'green']  # Red for misclassified, Green for recognized\n",
    "        )\n",
    "\n",
    "        # For Toxicity Classification\n",
    "        toxicity_predictions = np.array(val_predictions_toxicity)\n",
    "        toxicity_targets = np.array(val_targets_toxicity)\n",
    "\n",
    "        # Plot Confusion Matrix for Toxicity Classification\n",
    "        conf_matrix(\n",
    "            toxicity_targets, \n",
    "            toxicity_predictions, \n",
    "            task_name='Toxicity', \n",
    "            matrix_type='toxicity', \n",
    "            colors=['orange', 'yellow', 'purple', 'black']  # TN, FP, FN, TP\n",
    "        )\n",
    "        \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies_species': train_accuracies_species,\n",
    "        'train_accuracies_toxicity': train_accuracies_toxicity,\n",
    "        'val_accuracies_species': val_accuracies_species,\n",
    "        'val_accuracies_toxicity': val_accuracies_toxicity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train Batch 1/644: Loss = 4.9902, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 2/644: Loss = 5.2633, Species Acc = 0.00%, Toxicity Acc = 78.12%\n",
      "Train Batch 3/644: Loss = 4.6822, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 4/644: Loss = 5.0829, Species Acc = 3.12%, Toxicity Acc = 78.12%\n",
      "Train Batch 5/644: Loss = 4.8650, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 6/644: Loss = 4.8097, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 7/644: Loss = 4.6622, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 8/644: Loss = 4.8120, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 9/644: Loss = 4.9224, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 10/644: Loss = 5.1301, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 11/644: Loss = 4.9194, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 12/644: Loss = 5.1352, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 13/644: Loss = 5.0117, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 14/644: Loss = 5.0293, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 15/644: Loss = 4.8247, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 16/644: Loss = 4.7296, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 17/644: Loss = 4.9495, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 18/644: Loss = 4.8590, Species Acc = 3.12%, Toxicity Acc = 100.00%\n",
      "Train Batch 19/644: Loss = 4.9578, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 20/644: Loss = 4.8565, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 21/644: Loss = 4.7576, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 22/644: Loss = 5.0948, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 23/644: Loss = 4.9376, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 24/644: Loss = 4.9250, Species Acc = 3.12%, Toxicity Acc = 87.50%\n",
      "Train Batch 25/644: Loss = 4.9216, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 26/644: Loss = 4.9724, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 27/644: Loss = 5.0400, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 28/644: Loss = 5.1228, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 29/644: Loss = 4.9032, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 30/644: Loss = 5.1451, Species Acc = 0.00%, Toxicity Acc = 78.12%\n",
      "Train Batch 31/644: Loss = 5.0174, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 32/644: Loss = 4.9245, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 33/644: Loss = 4.8293, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 34/644: Loss = 5.0969, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 35/644: Loss = 4.9002, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 36/644: Loss = 4.7273, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 37/644: Loss = 5.0495, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 38/644: Loss = 4.9714, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 39/644: Loss = 4.8946, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 40/644: Loss = 4.7967, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 41/644: Loss = 4.8266, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 42/644: Loss = 4.6852, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 43/644: Loss = 4.8495, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 44/644: Loss = 4.9730, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 45/644: Loss = 5.0594, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 46/644: Loss = 4.8390, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 47/644: Loss = 4.8039, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 48/644: Loss = 4.9672, Species Acc = 3.12%, Toxicity Acc = 81.25%\n",
      "Train Batch 49/644: Loss = 4.7042, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 50/644: Loss = 4.8075, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 51/644: Loss = 4.7508, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 52/644: Loss = 4.7781, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 53/644: Loss = 4.8545, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 54/644: Loss = 4.7191, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 55/644: Loss = 4.8693, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 56/644: Loss = 4.8224, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 57/644: Loss = 4.6058, Species Acc = 6.25%, Toxicity Acc = 96.88%\n",
      "Train Batch 58/644: Loss = 4.6589, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 59/644: Loss = 4.9921, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 60/644: Loss = 4.7834, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 61/644: Loss = 5.1543, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 62/644: Loss = 4.9279, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 63/644: Loss = 4.7972, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 64/644: Loss = 4.8151, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 65/644: Loss = 4.8883, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 66/644: Loss = 5.0102, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 67/644: Loss = 4.7164, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 68/644: Loss = 4.7401, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 69/644: Loss = 4.8171, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 70/644: Loss = 4.7415, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 71/644: Loss = 4.9348, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 72/644: Loss = 4.8268, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 73/644: Loss = 4.9420, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 74/644: Loss = 4.8402, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 75/644: Loss = 4.7547, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 76/644: Loss = 4.7813, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 77/644: Loss = 4.8082, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 78/644: Loss = 4.5555, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 79/644: Loss = 4.9271, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 80/644: Loss = 4.9503, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 81/644: Loss = 5.0125, Species Acc = 3.12%, Toxicity Acc = 78.12%\n",
      "Train Batch 82/644: Loss = 4.7676, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 83/644: Loss = 4.9417, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 84/644: Loss = 4.8078, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 85/644: Loss = 4.7379, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 86/644: Loss = 4.9884, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 87/644: Loss = 4.8705, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 88/644: Loss = 4.9630, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 89/644: Loss = 4.8857, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 90/644: Loss = 4.8398, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 91/644: Loss = 4.8211, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 92/644: Loss = 4.9883, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 93/644: Loss = 4.8835, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 94/644: Loss = 4.8757, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 95/644: Loss = 4.8425, Species Acc = 6.25%, Toxicity Acc = 90.62%\n",
      "Train Batch 96/644: Loss = 4.8257, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 97/644: Loss = 4.7067, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 98/644: Loss = 4.9335, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 99/644: Loss = 4.6806, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 100/644: Loss = 4.7902, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 101/644: Loss = 4.7867, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 102/644: Loss = 4.8636, Species Acc = 3.12%, Toxicity Acc = 87.50%\n",
      "Train Batch 103/644: Loss = 4.7737, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 104/644: Loss = 4.9242, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 105/644: Loss = 4.9372, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 106/644: Loss = 4.7722, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 107/644: Loss = 4.7878, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 108/644: Loss = 4.7694, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 109/644: Loss = 4.7472, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 110/644: Loss = 4.8119, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 111/644: Loss = 4.7893, Species Acc = 6.25%, Toxicity Acc = 93.75%\n",
      "Train Batch 112/644: Loss = 4.6474, Species Acc = 3.12%, Toxicity Acc = 100.00%\n",
      "Train Batch 113/644: Loss = 4.9070, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 114/644: Loss = 4.7839, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 115/644: Loss = 4.9195, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 116/644: Loss = 4.8941, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 117/644: Loss = 4.8109, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 118/644: Loss = 4.8946, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 119/644: Loss = 4.6421, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 120/644: Loss = 4.8346, Species Acc = 6.25%, Toxicity Acc = 87.50%\n",
      "Train Batch 121/644: Loss = 4.7163, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 122/644: Loss = 4.8126, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 123/644: Loss = 4.8007, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 124/644: Loss = 4.8919, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 125/644: Loss = 4.8338, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 126/644: Loss = 4.9338, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 127/644: Loss = 4.7629, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 128/644: Loss = 4.7837, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 129/644: Loss = 4.8896, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 130/644: Loss = 4.9239, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 131/644: Loss = 4.8073, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 132/644: Loss = 4.7719, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 133/644: Loss = 4.8473, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 134/644: Loss = 4.7987, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 135/644: Loss = 4.6666, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 136/644: Loss = 4.8434, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 137/644: Loss = 4.7903, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 138/644: Loss = 4.7304, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 139/644: Loss = 4.7887, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 140/644: Loss = 4.8396, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 141/644: Loss = 4.7580, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 142/644: Loss = 4.8423, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 143/644: Loss = 4.9785, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 144/644: Loss = 4.7517, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 145/644: Loss = 4.6667, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 146/644: Loss = 4.8042, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 147/644: Loss = 4.7991, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 148/644: Loss = 4.6818, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 149/644: Loss = 4.9468, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 150/644: Loss = 4.6837, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 151/644: Loss = 4.8561, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 152/644: Loss = 4.6991, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 153/644: Loss = 4.8190, Species Acc = 6.25%, Toxicity Acc = 87.50%\n",
      "Train Batch 154/644: Loss = 4.8472, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 155/644: Loss = 5.0946, Species Acc = 0.00%, Toxicity Acc = 78.12%\n",
      "Train Batch 156/644: Loss = 4.8059, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 157/644: Loss = 4.7711, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 158/644: Loss = 4.8105, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 159/644: Loss = 4.8041, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 160/644: Loss = 4.7265, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 161/644: Loss = 4.7909, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 162/644: Loss = 4.9332, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 163/644: Loss = 4.8056, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 164/644: Loss = 4.7987, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 165/644: Loss = 4.9058, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 166/644: Loss = 4.9124, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 167/644: Loss = 4.8632, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 168/644: Loss = 4.7769, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 169/644: Loss = 4.8177, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 170/644: Loss = 4.7417, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 171/644: Loss = 4.7897, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 172/644: Loss = 4.8160, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 173/644: Loss = 4.6577, Species Acc = 6.25%, Toxicity Acc = 96.88%\n",
      "Train Batch 174/644: Loss = 4.7299, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 175/644: Loss = 4.8152, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 176/644: Loss = 4.8034, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 177/644: Loss = 4.7611, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 178/644: Loss = 4.7727, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 179/644: Loss = 4.8189, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 180/644: Loss = 4.7394, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 181/644: Loss = 4.7882, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 182/644: Loss = 4.6935, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 183/644: Loss = 4.7654, Species Acc = 6.25%, Toxicity Acc = 90.62%\n",
      "Train Batch 184/644: Loss = 4.7963, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 185/644: Loss = 4.7892, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 186/644: Loss = 4.7113, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 187/644: Loss = 4.7747, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 188/644: Loss = 4.9443, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 189/644: Loss = 4.7806, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 190/644: Loss = 4.8427, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 191/644: Loss = 5.0288, Species Acc = 0.00%, Toxicity Acc = 75.00%\n",
      "Train Batch 192/644: Loss = 4.7639, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 193/644: Loss = 4.9568, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 194/644: Loss = 4.8711, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 195/644: Loss = 4.7159, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 196/644: Loss = 4.8927, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 197/644: Loss = 4.7481, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 198/644: Loss = 4.8713, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 199/644: Loss = 4.8294, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 200/644: Loss = 4.8425, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 201/644: Loss = 4.8029, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 202/644: Loss = 4.7928, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 203/644: Loss = 4.6914, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 204/644: Loss = 4.8680, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 205/644: Loss = 4.8283, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 206/644: Loss = 4.8257, Species Acc = 6.25%, Toxicity Acc = 90.62%\n",
      "Train Batch 207/644: Loss = 4.7513, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 208/644: Loss = 4.7435, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 209/644: Loss = 4.8144, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 210/644: Loss = 4.8232, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 211/644: Loss = 4.7461, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 212/644: Loss = 4.7315, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 213/644: Loss = 4.7030, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 214/644: Loss = 4.7476, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 215/644: Loss = 4.9329, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 216/644: Loss = 4.7872, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 217/644: Loss = 4.8508, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 218/644: Loss = 4.7013, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 219/644: Loss = 4.9419, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 220/644: Loss = 4.9003, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 221/644: Loss = 4.8046, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 222/644: Loss = 4.8078, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 223/644: Loss = 4.9628, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 224/644: Loss = 4.9035, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 225/644: Loss = 4.8168, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 226/644: Loss = 4.7790, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 227/644: Loss = 4.6755, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 228/644: Loss = 4.7546, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 229/644: Loss = 4.7873, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 230/644: Loss = 4.8392, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 231/644: Loss = 4.8077, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 232/644: Loss = 4.8435, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 233/644: Loss = 4.8354, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 234/644: Loss = 4.7299, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 235/644: Loss = 4.6717, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 236/644: Loss = 4.6225, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 237/644: Loss = 4.7918, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 238/644: Loss = 4.7277, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 239/644: Loss = 4.9809, Species Acc = 3.12%, Toxicity Acc = 87.50%\n",
      "Train Batch 240/644: Loss = 4.9798, Species Acc = 3.12%, Toxicity Acc = 81.25%\n",
      "Train Batch 241/644: Loss = 4.8181, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 242/644: Loss = 4.7637, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 243/644: Loss = 4.8361, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 244/644: Loss = 4.8493, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 245/644: Loss = 4.8093, Species Acc = 3.12%, Toxicity Acc = 87.50%\n",
      "Train Batch 246/644: Loss = 4.7515, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 247/644: Loss = 4.8382, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 248/644: Loss = 4.7196, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 249/644: Loss = 4.8747, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 250/644: Loss = 4.7025, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 251/644: Loss = 4.8310, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 252/644: Loss = 4.9112, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 253/644: Loss = 4.7321, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 254/644: Loss = 4.8419, Species Acc = 6.25%, Toxicity Acc = 90.62%\n",
      "Train Batch 255/644: Loss = 4.8375, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 256/644: Loss = 4.7371, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 257/644: Loss = 4.8642, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 258/644: Loss = 4.7335, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 259/644: Loss = 4.7546, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 260/644: Loss = 4.7042, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 261/644: Loss = 4.8455, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 262/644: Loss = 4.7918, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 263/644: Loss = 4.8349, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 264/644: Loss = 4.6602, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 265/644: Loss = 4.7264, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 266/644: Loss = 4.7480, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 267/644: Loss = 4.7459, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 268/644: Loss = 4.9454, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 269/644: Loss = 4.8884, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 270/644: Loss = 4.9044, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 271/644: Loss = 4.6222, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 272/644: Loss = 4.7542, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 273/644: Loss = 4.8227, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 274/644: Loss = 4.7843, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 275/644: Loss = 4.8347, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 276/644: Loss = 4.8118, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 277/644: Loss = 4.8725, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 278/644: Loss = 4.7360, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 279/644: Loss = 4.8115, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 280/644: Loss = 4.7823, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 281/644: Loss = 4.7122, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 282/644: Loss = 4.6897, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 283/644: Loss = 4.5872, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 284/644: Loss = 4.7912, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 285/644: Loss = 4.9102, Species Acc = 6.25%, Toxicity Acc = 84.38%\n",
      "Train Batch 286/644: Loss = 4.8615, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 287/644: Loss = 4.8549, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 288/644: Loss = 4.9527, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 289/644: Loss = 4.8081, Species Acc = 6.25%, Toxicity Acc = 84.38%\n",
      "Train Batch 290/644: Loss = 4.9268, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 291/644: Loss = 4.9415, Species Acc = 0.00%, Toxicity Acc = 81.25%\n",
      "Train Batch 292/644: Loss = 4.8036, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 293/644: Loss = 4.7890, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 294/644: Loss = 4.9487, Species Acc = 3.12%, Toxicity Acc = 78.12%\n",
      "Train Batch 295/644: Loss = 4.8014, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 296/644: Loss = 4.8336, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 297/644: Loss = 4.9285, Species Acc = 3.12%, Toxicity Acc = 87.50%\n",
      "Train Batch 298/644: Loss = 4.7125, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 299/644: Loss = 4.8495, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 300/644: Loss = 4.9044, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 301/644: Loss = 4.7263, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 302/644: Loss = 4.7590, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 303/644: Loss = 4.6960, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 304/644: Loss = 4.7054, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 305/644: Loss = 4.8653, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 306/644: Loss = 4.9927, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 307/644: Loss = 4.7777, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 308/644: Loss = 4.7853, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 309/644: Loss = 4.9431, Species Acc = 3.12%, Toxicity Acc = 81.25%\n",
      "Train Batch 310/644: Loss = 4.8422, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 311/644: Loss = 4.7271, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 312/644: Loss = 4.8313, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 313/644: Loss = 4.8403, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 314/644: Loss = 4.7373, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 315/644: Loss = 4.8506, Species Acc = 3.12%, Toxicity Acc = 84.38%\n",
      "Train Batch 316/644: Loss = 4.7237, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 317/644: Loss = 4.7416, Species Acc = 3.12%, Toxicity Acc = 93.75%\n",
      "Train Batch 318/644: Loss = 4.7089, Species Acc = 0.00%, Toxicity Acc = 100.00%\n",
      "Train Batch 319/644: Loss = 4.7473, Species Acc = 0.00%, Toxicity Acc = 93.75%\n",
      "Train Batch 320/644: Loss = 4.7963, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 321/644: Loss = 4.8024, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 322/644: Loss = 4.9358, Species Acc = 0.00%, Toxicity Acc = 84.38%\n",
      "Train Batch 323/644: Loss = 4.6235, Species Acc = 3.12%, Toxicity Acc = 96.88%\n",
      "Train Batch 324/644: Loss = 4.7314, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 325/644: Loss = 4.8463, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 326/644: Loss = 4.9186, Species Acc = 0.00%, Toxicity Acc = 87.50%\n",
      "Train Batch 327/644: Loss = 4.8186, Species Acc = 0.00%, Toxicity Acc = 90.62%\n",
      "Train Batch 328/644: Loss = 4.8433, Species Acc = 3.12%, Toxicity Acc = 90.62%\n",
      "Train Batch 329/644: Loss = 4.6165, Species Acc = 3.12%, Toxicity Acc = 100.00%\n",
      "Train Batch 330/644: Loss = 4.6524, Species Acc = 0.00%, Toxicity Acc = 96.88%\n",
      "Train Batch 331/644: Loss = 4.8889, Species Acc = 0.00%, Toxicity Acc = 90.62%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, num_epochs, alpha, beta)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss_toxicity \u001b[38;5;241m=\u001b[39m criterion_toxicity(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoxicity\u001b[39m\u001b[38;5;124m'\u001b[39m], toxicities)\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m loss_species \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m loss_toxicity  \u001b[38;5;66;03m# weighting\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Update running loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Egyetem/Adatelemzes/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Egyetem/Adatelemzes/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "training_stats = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "# Save the model and statistics\n",
    "torch.save(model.state_dict(), \"multi_fungi_model.pth\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), training_stats['train_losses'], label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), training_stats['val_losses'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Species Classification Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), training_stats['train_accuracies_species'], label=\"Train Species Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), training_stats['val_accuracies_species'], label=\"Validation Species Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Species Classification Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Toxicity Classification Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), training_stats['train_accuracies_toxicity'], label=\"Train Toxicity Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), training_stats['val_accuracies_toxicity'], label=\"Validation Toxicity Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Toxicity Classification Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot User-Focused Loss (if included)\n",
    "if 'val_user_focused_losses' in training_stats:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), training_stats['val_user_focused_losses'], label=\"Validation User-Focused Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Validation User-Focused Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
